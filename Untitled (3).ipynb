{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08df5d7f-6b74-4f74-bb55-b0dc4f741c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linear Regression ===\n",
      "True weights: [-1.30652685  1.65813068 -0.11816405]\n",
      "Estimated weights: [-1.21268856  1.56880831 -0.16955712]\n",
      "\n",
      "=== Logistic Regression ===\n",
      "Accuracy: 1.00\n",
      "\n",
      "=== Decision Tree ===\n",
      "Accuracy: 1.00\n",
      "\n",
      "=== Random Forest ===\n",
      "Accuracy: 1.00\n",
      "\n",
      "=== K-Means ===\n",
      "Cluster sizes: [100 100 100]\n",
      "\n",
      "=== PCA ===\n",
      "Explained variance: [27.06674361  0.99887025]\n",
      "\n",
      "=== Gaussian Mixture ===\n",
      "Cluster sizes: [102  98 100]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import log, exp\n",
    "from collections import Counter\n",
    "from scipy.special import logsumexp\n",
    "def make_regression_data(n=200, d=1, noise=1.0, random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    X = rng.randn(n, d)\n",
    "    w = rng.randn(d,)\n",
    "    y = X.dot(w) + noise * rng.randn(n)\n",
    "    return X, y, w\n",
    "\n",
    "def make_classification_data(n=200, d=2, random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    X0 = rng.randn(n//2, d) + np.array([-2.0]*d)\n",
    "    X1 = rng.randn(n//2, d) + np.array([2.0]*d)\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.array([0]*(n//2) + [1]*(n//2))\n",
    "    perm = rng.permutation(n)\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "def make_cluster_data(n=300, centers=3, random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    X = rng.randn(n, 2)\n",
    "    for i in range(centers):\n",
    "        X[i*(n//centers):(i+1)*(n//centers)] += rng.randn(2)*3 + i*5\n",
    "    return X\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, method='closed-form', lr=0.01, n_iters=1000, lam=0.0):\n",
    "        self.method = method\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.lam = lam  \n",
    "        self.w = None\n",
    "        self.b = 0.0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.method == 'closed-form':\n",
    "            X_b = np.c_[np.ones(X.shape[0]), X]\n",
    "            A = X_b.T.dot(X_b) + self.lam * np.eye(X_b.shape[1])\n",
    "            self.w = np.linalg.solve(A, X_b.T.dot(y))\n",
    "            self.b = self.w[0]\n",
    "            self.w = self.w[1:]\n",
    "        else:\n",
    "            n, d = X.shape\n",
    "            self.w = np.zeros(d)\n",
    "            for _ in range(self.n_iters):\n",
    "                y_pred = X.dot(self.w) + self.b\n",
    "                dw = (X.T.dot(y_pred - y) + self.lam * self.w) / n\n",
    "                db = np.mean(y_pred - y)\n",
    "                self.w -= self.lr * dw\n",
    "                self.b -= self.lr * db\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X.dot(self.w) + self.b\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, n_iters=1000, lam=0.0):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.lam = lam\n",
    "        self.w = None\n",
    "        self.b = 0.0\n",
    "        \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape\n",
    "        self.w = np.zeros(d)\n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            z = X.dot(self.w) + self.b\n",
    "            p = self._sigmoid(z)\n",
    "            dw = (X.T.dot(p - y) + self.lam * self.w) / n\n",
    "            db = np.mean(p - y)\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self._sigmoid(X.dot(self.w) + self.b)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "\n",
    "class DecisionTree:\n",
    "    class Node:\n",
    "        def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):\n",
    "            self.feature_idx = feature_idx\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "    \n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "        \n",
    "    def _gini(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probs = counts / len(y)\n",
    "        return 1 - np.sum(probs**2)\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        best_gini = float('inf')\n",
    "        best_idx, best_thr = None, None\n",
    "        \n",
    "        for idx in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, idx])\n",
    "            for thr in thresholds:\n",
    "                left_mask = X[:, idx] <= thr\n",
    "                gini = (left_mask.sum() * self._gini(y[left_mask]) + \n",
    "                        (~left_mask).sum() * self._gini(y[~left_mask])) / len(y)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = thr\n",
    "        return best_idx, best_thr\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            n_classes == 1):\n",
    "            return self.Node(value=np.argmax(np.bincount(y)))\n",
    "        \n",
    "        idx, thr = self._best_split(X, y)\n",
    "        if idx is None:\n",
    "            return self.Node(value=np.argmax(np.bincount(y)))\n",
    "        \n",
    "        left_mask = X[:, idx] <= thr\n",
    "        left = self._build_tree(X[left_mask], y[left_mask], depth+1)\n",
    "        right = self._build_tree(X[~left_mask], y[~left_mask], depth+1)\n",
    "        return self.Node(idx, thr, left, right)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _predict_one(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self._predict_one(x, node.left)\n",
    "        else:\n",
    "            return self._predict_one(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.root) for x in X])\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=2, \n",
    "                 max_features='sqrt', random_state=0):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "        self.trees = []\n",
    "        \n",
    "    def _get_max_features(self, n_features):\n",
    "        if self.max_features == 'sqrt':\n",
    "            return int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            return int(np.log2(n_features)) + 1\n",
    "        else:\n",
    "            return n_features\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        max_features = self._get_max_features(n_features)\n",
    "        \n",
    "        for _ in range(self.n_trees):\n",
    "            idxs = self.rng.choice(n_samples, n_samples, replace=True)\n",
    "            feat_idxs = self.rng.choice(n_features, max_features, replace=False)\n",
    "            \n",
    "            tree = DecisionTree(max_depth=self.max_depth, \n",
    "                               min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X[idxs][:, feat_idxs], y[idxs])\n",
    "            self.trees.append((tree, feat_idxs))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        all_preds = []\n",
    "        for tree, feat_idxs in self.trees:\n",
    "            preds = tree.predict(X[:, feat_idxs])\n",
    "            all_preds.append(preds)\n",
    "        return np.array([Counter(col).most_common(1)[0][0] \n",
    "                        for col in np.array(all_preds).T])\n",
    "class KMeans:\n",
    "    def __init__(self, k=3, max_iters=100, random_state=0):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "        self.centroids = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        centroid_idxs = self.rng.choice(n_samples, self.k, replace=False)\n",
    "        self.centroids = X[centroid_idxs]\n",
    "        \n",
    "        for _ in range(self.max_iters):\n",
    "            distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "            labels = np.argmin(distances, axis=0)\n",
    "            new_centroids = np.array([X[labels == k].mean(axis=0) \n",
    "                                    for k in range(self.k)])\n",
    "            if np.allclose(self.centroids, new_centroids):\n",
    "                break\n",
    "                \n",
    "            self.centroids = new_centroids\n",
    "        \n",
    "        self.labels_ = labels\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "        return np.argmin(distances, axis=0)\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "        cov = np.cov(X_centered, rowvar=False)\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "        \n",
    "        idxs = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvectors = eigenvectors[:, idxs]\n",
    "        eigenvalues = eigenvalues[idxs]\n",
    "        if self.n_components is not None:\n",
    "            self.components = eigenvectors[:, :self.n_components]\n",
    "        else:\n",
    "            self.components = eigenvectors\n",
    "            \n",
    "        self.explained_variance_ = eigenvalues\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        return X_centered.dot(self.components)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "class GaussianMixture:\n",
    "    def __init__(self, n_components=3, max_iters=100, tol=1e-4, random_state=0):\n",
    "        self.n_components = n_components\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "        \n",
    "    def _multivariate_normal_pdf(self, X, mean, cov):\n",
    "        n = X.shape[1]\n",
    "        diff = X - mean\n",
    "        cov_inv = np.linalg.inv(cov)\n",
    "        numerator = np.exp(-0.5 * np.sum(diff @ cov_inv * diff, axis=1))\n",
    "        denominator = np.sqrt((2 * np.pi)**n * np.linalg.det(cov))\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights_ = np.ones(self.n_components) / self.n_components\n",
    "        self.means_ = X[self.rng.choice(n_samples, self.n_components, replace=False)]\n",
    "        self.covariances_ = np.array([np.cov(X.T) + 1e-6*np.eye(n_features) \n",
    "                                    for _ in range(self.n_components)])\n",
    "        \n",
    "        log_likelihood = None\n",
    "        \n",
    "        for _ in range(self.max_iters):\n",
    "            resp = np.zeros((n_samples, self.n_components))\n",
    "            for k in range(self.n_components):\n",
    "                resp[:, k] = self.weights_[k] * self._multivariate_normal_pdf(\n",
    "                    X, self.means_[k], self.covariances_[k])\n",
    "            resp /= resp.sum(axis=1, keepdims=True)\n",
    "            \n",
    "            Nk = resp.sum(axis=0)\n",
    "            self.weights_ = Nk / n_samples\n",
    "            \n",
    "            for k in range(self.n_components):\n",
    "                self.means_[k] = (resp[:, k] @ X) / Nk[k]\n",
    "                diff = X - self.means_[k]\n",
    "                self.covariances_[k] = (resp[:, k] * diff.T) @ diff / Nk[k]\n",
    "                self.covariances_[k] += 1e-6 * np.eye(n_features)  \n",
    "            new_log_likelihood = 0\n",
    "            for k in range(self.n_components):\n",
    "                new_log_likelihood += np.log(self.weights_[k] * \n",
    "                                           self._multivariate_normal_pdf(\n",
    "                                               X, self.means_[k], self.covariances_[k])).sum()\n",
    "            \n",
    "            if log_likelihood is not None and abs(new_log_likelihood - log_likelihood) < self.tol:\n",
    "                break\n",
    "            log_likelihood = new_log_likelihood\n",
    "        \n",
    "        self.labels_ = np.argmax(resp, axis=1)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probas = np.zeros((X.shape[0], self.n_components))\n",
    "        for k in range(self.n_components):\n",
    "            probas[:, k] = self.weights_[k] * self._multivariate_normal_pdf(\n",
    "                X, self.means_[k], self.covariances_[k])\n",
    "        return probas / probas.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "def demo():\n",
    "    print(\"=== Linear Regression\")\n",
    "    X, y, true_w = make_regression_data(n=100, d=3)\n",
    "    lr = LinearRegression(method='closed-form').fit(X, y)\n",
    "    print(f\"True weights: {true_w}\")\n",
    "    print(f\"Estimated weights: {lr.w}\")\n",
    "    \n",
    "    print(\"\\n=== Logistic Regression ===\")\n",
    "    Xc, yc = make_classification_data(n=100)\n",
    "    logreg = LogisticRegression().fit(Xc, yc)\n",
    "    print(f\"Accuracy: {(logreg.predict(Xc) == yc).mean():.2f}\")\n",
    "    \n",
    "    print(\"\\n=== Decision Tree ===\")\n",
    "    tree = DecisionTree(max_depth=3).fit(Xc, yc)\n",
    "    print(f\"Accuracy: {(tree.predict(Xc) == yc).mean():.2f}\")\n",
    "    \n",
    "    print(\"\\n=== Random Forest ===\")\n",
    "    rf = RandomForest(n_trees=10).fit(Xc, yc)\n",
    "    print(f\"Accuracy: {(rf.predict(Xc) == yc).mean():.2f}\")\n",
    "    \n",
    "    print(\"\\n=== K-Means ===\")\n",
    "    Xk = make_cluster_data(n=300, centers=3)\n",
    "    km = KMeans(k=3).fit(Xk)\n",
    "    print(\"Cluster sizes:\", np.bincount(km.labels_))\n",
    "    \n",
    "    print(\"\\n=== PCA ===\")\n",
    "    pca = PCA(n_components=2).fit(Xk)\n",
    "    print(\"Explained variance:\", pca.explained_variance_)\n",
    "    \n",
    "    print(\"\\n=== Gaussian Mixture ===\")\n",
    "    gmm = GaussianMixture(n_components=3).fit(Xk)\n",
    "    print(\"Cluster sizes:\", np.bincount(gmm.predict(Xk)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
